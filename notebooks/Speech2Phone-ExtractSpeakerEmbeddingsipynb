{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a noteboook used to generate the speaker embeddings with the  Speech2Phone Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip uninstall pydub -y\n",
    "!conda remove pydub -y\n",
    "!conda install -c conda-forge pydub -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Speech2Phone Requeriments\n",
    "! pip install tensorflow==1.14.0 tflearn==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Speech2Phone Checkpoint\n",
    "!wget -O ./saver.zip https://www.dropbox.com/s/b19xt2wu3th9p36/Save-Models-Speaker-Diarization.zip?dl=0\n",
    "!mkdir Speech2Phone\n",
    "!unzip saver.zip\n",
    "!mv  Save-Models/  Speech2Phone/Save-Models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils for Speech2Phone Preprocessing\n",
    "from pydub import AudioSegment as audio\n",
    "\n",
    "def detect_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):\n",
    "    '''\n",
    "    sound is a pydub.AudioSegment\n",
    "    silence_threshold in dB\n",
    "    chunk_size in ms\n",
    " \n",
    "    iterate over chunks until you find the first one with sound\n",
    "    '''\n",
    "    trim_ms = 0  # ms\n",
    "    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold:\n",
    "        #print(trim_ms,len(sound))\n",
    "        if trim_ms > len(sound):\n",
    "            return None\n",
    "        trim_ms += chunk_size\n",
    " \n",
    "    return trim_ms\n",
    "\n",
    "def remove_silence(sound):\n",
    "    start_trim = detect_leading_silence(sound)\n",
    "    if start_trim is None:\n",
    "        return None\n",
    "    end_trim = detect_leading_silence(sound.reverse())\n",
    "    duration = len(sound)\n",
    "    trimmed_sound = sound[start_trim:duration-end_trim]\n",
    "    return trimmed_sound\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "\n",
    "#Create model for restore\n",
    "encoder = tflearn.input_data(shape=[None, 13,int(216)])\n",
    "encoder = tflearn.dropout(encoder,0.9) #10 % drop - 90% -> 80\n",
    "encoder = tflearn.dropout(encoder,0.2)# 80 % drop\n",
    "encoder = tflearn.fully_connected(encoder, 40,activation='crelu')\n",
    "decoder = tflearn.fully_connected(encoder, int(572), activation='linear')\n",
    "net = tflearn.regression(decoder, optimizer='adam', learning_rate=0.0007,loss='mean_square', metric=None)#categorical_crossentropy\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0,tensorboard_dir='tflearn_logs')\n",
    "\n",
    "model.load('./Speech2Phone/Save-Models/Model3-Best-40loc.tflearn')\n",
    "\n",
    "encoding_model = tflearn.DNN(encoder, session=model.session)# used for extract embedding in encoder layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "DATA_ROOT_PATH = '../../../LibriSpeech/voicefilter_data-3/'\n",
    "TRAIN_DATA = os.path.join(DATA_ROOT_PATH, 'train')\n",
    "TEST_DATA = os.path.join(DATA_ROOT_PATH, 'test')\n",
    "glob_re_wav_emb = '*-ref_emb.wav'\n",
    "glob_re_emb = '*-emb.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess dataset\n",
    "train_files = sorted(glob(os.path.join(TRAIN_DATA, glob_re_wav_emb)))\n",
    "test_files = sorted(glob(os.path.join(TEST_DATA, glob_re_wav_emb)))\n",
    "\n",
    "if len(train_files) == 0 or len(test_files):\n",
    "    print(\"check train and test path files not in directory\")\n",
    "files  = train_files+test_files\n",
    "      \n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    wave_file_path = files[i]\n",
    "    print(files)\n",
    "    wav_file_name = os.path.basename(wave_file_path)\n",
    "    # Extract Embedding\n",
    "    try:\n",
    "        sound = audio.from_wav(wave_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"erro ler arquivo\", e)\n",
    "        continue\n",
    "    wave = remove_silence(sound)\n",
    "    if wave is None:\n",
    "        print(\"erro remove silence\")\n",
    "        continue\n",
    "    \n",
    "    file_embeddings = None\n",
    "    begin = 0\n",
    "    end = 5\n",
    "    step = 1 \n",
    "    if int(wave.duration_seconds) < 5: # 5 seconds is the Speech2Phone input if is small concate\n",
    "        aux = wave\n",
    "        while int(aux.duration_seconds) <= 5:\n",
    "            aux += wave\n",
    "        wave = aux\n",
    "        del aux\n",
    "        \n",
    "    while (end) <= int(wave.duration_seconds):\n",
    "        try:        \n",
    "            segment = wave[begin*1000:end*1000]\n",
    "            segment.export('../aux' + '.wav', 'wav')# its necessary because pydub and librosa load wave in diferent form \n",
    "            y, sr = librosa.load('../aux.wav',sr=22050)#sample rate = 22050 \n",
    "            if file_embeddings is None:\n",
    "                file_embeddings =[np.array(encoding_model.predict([librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)])[0])]\n",
    "            else:\n",
    "                file_embeddings.append(np.array(encoding_model.predict([librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)])[0]))   \n",
    "            os.system('rm ../aux.wav')\n",
    "            begin = begin + step\n",
    "            end = end + step\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print('par, len(file_embeddings))\n",
    "            begin = begin + step\n",
    "            end = end + step\n",
    "    file_embedding = np.mean(np.array(file_embeddings), axis=0)\n",
    "    output_name = wave_file_path.replace(glob_re_wav_emb.replace('*',''),'')+glob_re_emb.replace('*','')\n",
    "    torch.save(torch.from_numpy(file_embedding.reshape(-1)), output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
