{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a noteboook used to generate the speaker embeddings with the  GE2E model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone encoder \n",
    "#!git clone https://github.com/CorentinJ/Real-Time-Voice-Cloning.git\n",
    "! git clone https://github.com/Edresson/GE2E-Speaker-Encoder.git\n",
    "os.chdir('GE2E-Speaker-Encoder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install voxceleb_trainer Requeriments\n",
    "!python -m pip install umap-learn visdom webrtcvad librosa>=0.5.1 matplotlib>=2.0.2 numpy>=1.14.0  scipy>=1.0.0  tqdm sounddevice Unidecode inflect multiprocess numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Download encoder Checkpoint\n",
    "!wget https://github.com/Edresson/Real-Time-Voice-Cloning/releases/download/checkpoints/pretrained.zip\n",
    "!unzip pretrained.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import inference as encoder\n",
    "from encoder.params_model import model_embedding_size as speaker_embedding_size\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n",
    "encoder.load_model(Path('encoder/saved_models/pretrained.pt'))\n",
    "print(\"Testing your configuration with small inputs.\")\n",
    "# Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's\n",
    "# sampling rate, which may differ.\n",
    "# If you're unfamiliar with digital audio, know that it is encoded as an array of floats \n",
    "# (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n",
    "# The sampling rate is the number of values (samples) recorded per second, it is set to\n",
    "# 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond \n",
    "# to an audio of 1 second.\n",
    "print(\"\\tTesting the encoder...\")\n",
    "\n",
    "wav = np.zeros(encoder.sampling_rate)    \n",
    "embed = encoder.embed_utterance(wav)\n",
    "print(embed.shape)\n",
    "\n",
    "# Embeddings are L2-normalized (this isn't important here, but if you want to make your own \n",
    "# embeddings it will be).\n",
    "#embed /= np.linalg.norm(embed) # for random embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "DATA_ROOT_PATH = '../../../../LibriSpeech/voicefilter_data/'\n",
    "TRAIN_DATA = os.path.join(DATA_ROOT_PATH, 'train')\n",
    "TEST_DATA = os.path.join(DATA_ROOT_PATH, 'test')\n",
    "glob_re_wav_emb = '*-ref_emb.wav'\n",
    "glob_re_emb = '*-emb.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess dataset\n",
    "train_files = sorted(glob(os.path.join(TRAIN_DATA, glob_re_wav_emb)))\n",
    "test_files = sorted(glob(os.path.join(TEST_DATA, glob_re_wav_emb)))\n",
    "\n",
    "if len(train_files) == 0 or len(test_files):\n",
    "    print(\"check train and test path files not in directory\")\n",
    "files  = train_files+test_files\n",
    "      \n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    wave_file_path = files[i]\n",
    "    wav_file_name = os.path.basename(wave_file_path)\n",
    "    # Extract Embedding\n",
    "    preprocessed_wav = encoder.preprocess_wav(wave_file_path)\n",
    "    file_embedding = encoder.embed_utterance(preprocessed_wav)\n",
    "    output_name = wave_file_path.replace(glob_re_wav_emb.replace('*',''),'')+glob_re_emb.replace('*','')\n",
    "    torch.save(torch.from_numpy(file_embedding.reshape(-1)), output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
